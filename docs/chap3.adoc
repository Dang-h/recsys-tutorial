== 机器学习模型简介

主要内容

*监督学习*

* 回归模型
** 线性回归

* 分类模型
** K近邻（KNN）
** 决策树
** 逻辑斯蒂回归

*无监督学习*

* 聚类
** K均值聚类

=== 线性回归

* 线性回归（linear regression）是一种线性模型，它假设输入变量x和单个输出变量y之间存在线性关系
* 具体来说，利用线性回归模型，可以从一组输入变量x的线性组合中，计算输出变量y

[stem]
++++
y=wx+b \\
f(x) = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
++++

image::xianxinghuigui.png[]

NOTE: 线性回归就是从上图中的一堆散点图，拟合出一条直线stem:[y=wx+b]。这里的拟合其实就是机器学习。散点为训练数据，我们要从训练数据中学习出一条直线。

这里的模型就是线性回归模型。那么机器学习的算法呢？

* 最小二乘法
* 梯度下降法

NOTE: 注意我们这里还没有讲监督学习三要素中的策略呢！

==== 损失函数

这里使用平方损失函数，并且没有正则化项。

[stem]
++++
Loss Function = \sum_{i=1}^m (wx_i + b - y_i)^2
++++

==== 学习算法

求解stem:[w,b]使得平方损失函数取得最小值。

* 最小二乘法
* 梯度下降法

==== 最小二乘法

* 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method）。
* 它的主要思想就是选择未知参数，使得理论值与观测值之差的平方和达到最小。

image::zuixiaoercheng.png[]

* 我们假设输入属性（特征）的数目只有一个：就是stem:[f(x)=wx+b]中的stem:[x]。我们的训练数据是stem:[(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)]。要从这些训练数据中学习一个模型出来，使得：stem:[f(x_i)=wx_i+b, f(x_i) \approx y_i]。

* 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。

[stem]
++++
\begin{align}
(w^*,b^*) &= \arg \min_{(w,b)}\sum_{i=1}^m (f(x_i)-y_i)^2 \\
          &= \arg \min_{(w,b)}\sum_{i=1}^m (wx_i+b-y_i)^2
\end{align}
++++

NOTE: 以上就是监督学习三要素中的策略！

===== 最小二乘法求解过程

* 求解stem:[w]和stem:[b]，使得stem:[E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2]最小化的过程，称为线性回归模型的“最小二乘估计”。

* 将stem:[E_{(w,b)}]分别对stem:[w]和stem:[b]进行求导，可以得到：

[stem]
++++
\begin{align}
\frac{\partial E_{(w,b)}}{\partial w} 
&= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&= 2(wx_1+b-y_1)x_1+ \dots +2(wx_m+b-y_m)x_m \\
&= 2(wx_1^2-(y_1-b)x_1) + \dots + 2(wx_m^2-(y_m-b)x_m) \\
&= 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\

\frac{\partial E_{(w,b)}}{b}
&= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&= 2(wx_1+b-y_1) + \dots + 2(wx_m+b-y_m) \\
&= 2(mb-\sum_{i=1}^m (y_i-wx_i))
\end{align}
++++

* 令偏导数都为0，可以得到：

[stem]
++++
2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) = 0 \\
w = \frac{\sum_{i=1}^m (y_i-b)x_i}{\sum_{i=1}^m x_i^2}
++++

将stem:[w]代入到第二个式子中，可以解得stem:[b]的值，然后再将stem:[b]的值代入到w的表达式，即可解得stem:[w]。

[stem]
++++
w = \frac{\sum_{i=1}^m y_i(x_i-\bar{x})}{\sum_{i=1}^m x_i^2-\frac{1}{m}(\sum_{i=1}^m x_i)^2} \\
b = \frac{1}{m}\sum_{i=1}^m (y_i-wx_i)
++++

其中：stem:[\bar{x}=\frac{1}{m}\sum_{i=1}^m x_i]

==== 梯度下降法求解过程

求解stem:[w]和stem:[b]，使得stem:[E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2]达到极小值，也可以使用梯度下降法。

梯度为：

[stem]
++++
\nabla E = 
[\frac{\partial}{\partial w} E_{(w,b)}, \frac{\partial}{\partial b} E_{(w,b)}] \\
\frac{\partial E_{(w,b)}}{\partial w} = 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\
\frac{\partial E_{(w,b)}}{b} = 2(mb-\sum_{i=1}^m (y_i-wx_i))
++++

NOTE: 这里不是令偏导数为零了，而是准备梯度下降。

[stem]
++++
w_{next} := w_{current} - \alpha \frac{\partial E}{\partial w} \\
b_{next} := b_{current} - \alpha \frac{\partial E}{\partial b}
++++

NOTE: 以上，stem:[w,b]的初始值，stem:[\alpha]的值，以及迭代的次数，都是超参数。因为不是机器学习学习出来的参数。

* stem:[\alpha]在梯度下降算法中被称作为学习率或者步长
* 这意味着我们可以通过stem:[\alpha]来控制每一步走的距离，以保证不要走太快，错过了最低点；同时也要保证收敛速度不要太慢
* 所以stem:[\alpha]的选择在梯度下降法中往往是很重要的，不能太大也不能太小

image::xuexilv.png[]

==== 梯度下降法和最小二乘法

* 相同点
** 本质和目标相同：两种方法都是经典的学习算法，在给定已知数据的前提下利用求导算出一个模型（函数），使得损失函数最小，然后对给定的新数据进行估算预测

* 不同点
** 损失函数：梯度下降可以选取其它损失函数，而最小二乘一定是平方损失函数
** 实现方法：最小二乘法是直接求导找出全局最小；而梯度下降是一种迭代法
** 效果：最小二乘找到的一定是全局最小，但计算繁琐，且复杂情况下未必有解；梯度下降迭代计算简单，但找到的一般是局部最小，只有在目标函数是凸函数时才是全局最小；到最小点附近时收敛速度会变慢，且对初始点的选择极为敏感

=== 感知机（Perceptron）

==== 感知机的定义

感知机是机器学习应用中分类的最简单的一种算法。如下图所示：感知机划分超平面

image::ganzhiji1.png[]

感知机是二分类的线性模型，输入是实例的特征向量，输出是实例的类别；假设训练的数据集是线性可分的，感知机的目标就是求一个能够将训练集的正负样本完全正确分离开的超平面(也就是上图中所示的那些将蓝、黄数据点完全分离开的直线)。但是如果这些数据是非线性可分的，这个超平面是无法获取的。上图的坐标轴，横坐标为stem:[X_1]，纵坐标为stem:[X_2]。图中的每一个点都由stem:[(X_1,X_2)]所决定。举个实例：有一批零件，判断零件是否合格有两个重要点，长度和重量。stem:[X_1]表示长度，stem:[X_2]表示重量，上图的两条黑线表示零件的长度均值和重量均值。只有当长度和重量都满足一定条件，该零件才为合格品。都不满足一定条件，视为不可修复的劣质品，直接丢弃。那么机器学习如何学习到这个规则呢？我们在代码实现的时候，拿到手的是所有样本的信息stem:[(X_1,X_2)]和标签(0或1)，标签里面0表示不合格品，1表示合格品。简单的说就是图片上黄色和蓝色的点。根据我们手上的这些点，我们需要找到一条直线将上面的点完美的分开。这样的直线我们可以找到很多条，那么哪一条才是最好的呢？实际上，感知机只是一个二分类的问题，无法找到一条最佳的直线，只需要能把所有的点都分开就好。我们设定损失函数为所有分错的点和直线的距离求和，然后训练，使这段求和的数值最小(最优的情况是0，因为0代表完全分开了)，那么这条直线就满足我们的条件，就是我们所找的。

==== 感知机的数学原理

首先，点stem:[P(x_0,y_0)]到直线stem:[Ax+By+C=0]的距离为：

[stem]
++++
d=\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}
++++

类似的：设超平面公式为：stem:[h=wx+b]，其中stem:[w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)]。其中样本点stem:[x']到超平面的距离为：

[stem]
++++
d=\frac{wx'+b}{\parallel w \parallel}
++++

那么这个超平面为什么设置为stem:[wx+b]呢？它和我们常见的stem:[ax+b]有什么区别呢？

本质没啥区别，stem:[ax+b]是二维中的，stem:[wx+b]是高维中的。就看你的理解啦，简单的来说，stem:[wx+b]是一个stem:[n]维空间中的超平面stem:[S]，其中stem:[w]是超平面的法向量，stem:[b]是超平面的截距，这个超平面将特征空间划分成两部分，位于两部分的点分别被分为正负两类，所以，超平面S称为分离超平面。其中stem:[w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)]。

细节：

stem:[w]是超平面的法向量：对于一个平面来说stem:[w]就是这么定义的。数学上就这么定义的。

stem:[b]是超平面的截距：可以按照二维中的stem:[ax+b]理解。

特征空间：也就是整个stem:[n]维空间，样本的每个属性都叫一个特征，特征空间的意思就是在这个空间中可以找到样本所有的属性组合。

==== 感知机的模型

image::ganzhiji2.png[]

感知机的模型：输入空间—>输出空间：

[stem]
++++
f(x)=sign(wx+b), 其中, sign(x)=
\begin{cases}
  -1 & x < 0 \\
  1 & x \ge 0
\end{cases}
++++

sign函数很简单，当x大于等于0，sign输出1，否则输出-1。那么往前想一下，stem:[wx+b]如果大于等于0，stem:[f(x)]就等于1，反之stem:[f(x)]等于-1。

==== 感知机的损失函数

我们定义样本stem:[(x_i,y_i)]，如果上面的距离stem:[d > 0]，则stem:[y_i=1]；如果stem:[d < 0],则stem:[y_i=-1]，这样取stem:[y]有一个好处，就是方便定义损失函数。优化的目标：期望使误分类的所有样本，到超平面的距离之和最小。

所以定义损失函数为：

[stem]
++++
L(w,b)=-\frac{1}{\parallel w \parallel} \sum_{x_i \in M}y_i(wx_i+b)
++++

其中M集合就是误分类点的集合。

不考虑前面的系数，感知机模型的损失函数为：

[stem]
++++
L(w,b)=-\sum_{x_i \in M}y_i(wx_i+b)
++++

==== 感知机学习算法

感知机学习算法是对于上述损失函数进行极小化，求得stem:[w]和stem:[b]。这里使用随机梯度下降法(SGD)，因为误分类的M集合里面的样本才能参加损失函数的优化。

目标函数如下：

[stem]
++++
L(w,b)=\arg \min_{w,b}(-\sum_{x_i \in M}y_i(wx_i+b))
++++

*算法步骤*

....
输入：训练数据集stem:[T=(x_N,y_N),(x_N,y_N),\dots,(x_N,y_N),y_i \in {-1,+1},学习率 \eta (0 < \eta < 1)]

输出：stem:[w,b]；感知机模型stem:[f(x)=sign(wx+b)]

. 赋初值stem:[w_0,b_0]
. 选取数据点stem:[(x_i,y_i)]
. 判断该数据点是否为当前模型的误分类点，即判断若stem:[y_i(wx_i+b) \le 0]则更新：

[stem]
++++
w = w + \eta y_i x_i \\
b = b + \eta y_i
++++

4. 转到2，直到训练集中没有误分类点。
....

==== 代码

https://zhuanlan.zhihu.com/p/32925500[点击链接学习如何安装和使用Anaconda]