<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.9">
<meta name="author" content="尚硅谷大数据讲师左元">
<title>尚硅谷机器学习和推荐系统教程</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
:not(pre)>code.nobreak{word-wrap:normal}
:not(pre)>code.nowrap{white-space:nowrap}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt,.quoteblock .quoteblock{margin:0 0 1.25em;padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
table.tableblock{max-width:100%;border-collapse:separate}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
td.tableblock>.content>:last-child.sidebarblock{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>尚硅谷机器学习和推荐系统教程</h1>
<div class="details">
<span id="author" class="author">尚硅谷大数据讲师左元</span><br>
<span id="email" class="email"><a href="mailto:zuoyuan@atguigu.com">zuoyuan@atguigu.com</a></span><br>
<span id="revnumber">version 1.0,</span>
<span id="revdate">2019-06-01</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_机器学习数学基础">1. 机器学习数学基础</a>
<ul class="sectlevel2">
<li><a href="#_线性代数">1.1. 线性代数</a>
<ul class="sectlevel3">
<li><a href="#_什么是矩阵">1.1.1. 什么是矩阵</a></li>
<li><a href="#_矩阵">1.1.2. 矩阵</a>
<ul class="sectlevel4">
<li><a href="#_矩阵的定义">矩阵的定义</a></li>
<li><a href="#_特殊矩阵">特殊矩阵</a></li>
</ul>
</li>
<li><a href="#_矩阵中的概念">1.1.3. 矩阵中的概念</a>
<ul class="sectlevel4">
<li><a href="#_特殊矩阵_2">特殊矩阵</a></li>
</ul>
</li>
<li><a href="#_矩阵的加法">1.1.4. 矩阵的加法</a></li>
<li><a href="#_矩阵的乘法">1.1.5. 矩阵的乘法</a>
<ul class="sectlevel4">
<li><a href="#_数与矩阵相乘">数与矩阵相乘</a></li>
<li><a href="#_矩阵与矩阵相乘">矩阵与矩阵相乘</a></li>
</ul>
</li>
<li><a href="#_矩阵的转置">1.1.6. 矩阵的转置</a></li>
<li><a href="#_矩阵的运算方法">1.1.7. 矩阵的运算方法</a>
<ul class="sectlevel4">
<li><a href="#_加法">加法</a></li>
<li><a href="#_乘法">乘法</a></li>
<li><a href="#_减法">减法</a></li>
<li><a href="#_转置">转置</a></li>
</ul>
</li>
<li><a href="#_矩阵的逆">1.1.8. 矩阵的逆</a></li>
</ul>
</li>
<li><a href="#_微积分基础知识">1.2. 微积分基础知识</a>
<ul class="sectlevel3">
<li><a href="#_什么是导数">1.2.1. 什么是导数</a></li>
<li><a href="#_偏导数">1.2.2. 偏导数</a></li>
<li><a href="#_方向导数">1.2.3. 方向导数</a></li>
<li><a href="#_梯度gradient">1.2.4. 梯度（Gradient）</a></li>
<li><a href="#_凸函数和凹函数">1.2.5. 凸函数和凹函数</a>
<ul class="sectlevel4">
<li><a href="#_凸函数">凸函数</a></li>
<li><a href="#_凹函数">凹函数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_概率论和统计学">1.3. 概率论和统计学</a>
<ul class="sectlevel3">
<li><a href="#_常用统计变量">1.3.1. 常用统计变量</a></li>
<li><a href="#_常见概率分布">1.3.2. 常见概率分布</a></li>
<li><a href="#_重要概率公式">1.3.3. 重要概率公式</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_数学基础精简进阶版">2. 数学基础(精简进阶版)</a>
<ul class="sectlevel2">
<li><a href="#_线性代数_2">2.1. 线性代数</a>
<ul class="sectlevel3">
<li><a href="#_向量">2.1.1. 向量</a></li>
<li><a href="#_矩阵_2">2.1.2. 矩阵</a></li>
<li><a href="#_运算">2.1.3. 运算</a></li>
<li><a href="#_范数">2.1.4. 范数</a></li>
<li><a href="#_特征向量和特征值">2.1.5. 特征向量和特征值</a></li>
</ul>
</li>
<li><a href="#_微分">2.2. 微分</a>
<ul class="sectlevel3">
<li><a href="#_导数和微分">2.2.1. 导数和微分</a></li>
<li><a href="#_泰勒展开">2.2.2. 泰勒展开</a></li>
<li><a href="#_偏导数_2">2.2.3. 偏导数</a></li>
<li><a href="#_梯度">2.2.4. 梯度</a></li>
<li><a href="#_海森矩阵">2.2.5. 海森矩阵</a></li>
</ul>
</li>
<li><a href="#_概率">2.3. 概率</a>
<ul class="sectlevel3">
<li><a href="#_条件概率">2.3.1. 条件概率</a></li>
<li><a href="#_期望">2.3.2. 期望</a></li>
<li><a href="#_均匀分布">2.3.3. 均匀分布</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_机器学习简介">3. 机器学习简介</a>
<ul class="sectlevel2">
<li><a href="#_机器学习的概念">3.1. 机器学习的概念</a>
<ul class="sectlevel3">
<li><a href="#_机器学习是什么">3.1.1. 机器学习是什么</a></li>
<li><a href="#_机器学习的开端">3.1.2. 机器学习的开端</a></li>
<li><a href="#_机器学习的定义">3.1.3. 机器学习的定义</a></li>
<li><a href="#_机器学习的过程">3.1.4. 机器学习的过程</a></li>
</ul>
</li>
<li><a href="#_机器学习的分类">3.2. 机器学习的分类</a>
<ul class="sectlevel3">
<li><a href="#_监督学习">3.2.1. 监督学习</a></li>
<li><a href="#_监督学习深入介绍">3.2.2. 监督学习深入介绍</a>
<ul class="sectlevel4">
<li><a href="#_监督学习三要素">监督学习三要素</a></li>
<li><a href="#_监督学习实现步骤">监督学习实现步骤</a></li>
<li><a href="#_监督学习过程示例">监督学习过程示例</a></li>
</ul>
</li>
<li><a href="#_模型评估策略">3.2.3. 模型评估策略</a>
<ul class="sectlevel4">
<li><a href="#_训练集和测试集">训练集和测试集</a></li>
<li><a href="#_损失函数">损失函数</a></li>
<li><a href="#_常见损失函数">常见损失函数</a></li>
<li><a href="#_经验风险">经验风险</a></li>
<li><a href="#_训练误差和测试误差">训练误差和测试误差</a></li>
<li><a href="#_过拟合和欠拟合">过拟合和欠拟合</a></li>
</ul>
</li>
<li><a href="#_模型的选择">3.2.4. 模型的选择</a></li>
<li><a href="#_正则化">3.2.5. 正则化</a>
<ul class="sectlevel4">
<li><a href="#_奥卡姆剃刀">奥卡姆剃刀</a></li>
</ul>
</li>
<li><a href="#_交叉验证">3.2.6. 交叉验证</a></li>
<li><a href="#_分类和回归">3.2.7. 分类和回归</a>
<ul class="sectlevel4">
<li><a href="#_分类问题">分类问题</a></li>
<li><a href="#_精确率和召回率">精确率和召回率</a></li>
<li><a href="#_回归问题">回归问题</a></li>
</ul>
</li>
<li><a href="#_模型求解算法学习算法">3.2.8. 模型求解算法（学习算法）</a>
<ul class="sectlevel4">
<li><a href="#_梯度下降算法">梯度下降算法</a></li>
</ul>
</li>
<li><a href="#_牛顿法和拟牛顿法">3.2.9. 牛顿法和拟牛顿法</a>
<ul class="sectlevel4">
<li><a href="#_牛顿法">牛顿法</a></li>
<li><a href="#_拟牛顿法">拟牛顿法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#_机器学习模型简介">4. 机器学习模型简介</a>
<ul class="sectlevel2">
<li><a href="#_线性回归">4.1. 线性回归</a>
<ul class="sectlevel3">
<li><a href="#_损失函数_2">4.1.1. 损失函数</a></li>
<li><a href="#_学习算法">4.1.2. 学习算法</a></li>
<li><a href="#_最小二乘法">4.1.3. 最小二乘法</a>
<ul class="sectlevel4">
<li><a href="#_最小二乘法求解过程">最小二乘法求解过程</a></li>
</ul>
</li>
<li><a href="#_梯度下降法求解过程">4.1.4. 梯度下降法求解过程</a></li>
<li><a href="#_梯度下降法和最小二乘法">4.1.5. 梯度下降法和最小二乘法</a></li>
</ul>
</li>
<li><a href="#_感知机perceptron">4.2. 感知机（Perceptron）</a>
<ul class="sectlevel3">
<li><a href="#_感知机的定义">4.2.1. 感知机的定义</a></li>
<li><a href="#_感知机的数学原理">4.2.2. 感知机的数学原理</a></li>
<li><a href="#_感知机的模型">4.2.3. 感知机的模型</a></li>
<li><a href="#_感知机的损失函数">4.2.4. 感知机的损失函数</a></li>
<li><a href="#_感知机学习算法">4.2.5. 感知机学习算法</a></li>
<li><a href="#_代码">4.2.6. 代码</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<img src="images/atguigu.jpg" alt="atguigu" width="300" height="200">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_机器学习数学基础">1. 机器学习数学基础</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>高等数学</p>
</li>
<li>
<p>线性代数</p>
</li>
<li>
<p>概率论和统计学</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_线性代数">1.1. 线性代数</h3>
<div class="sect3">
<h4 id="_什么是矩阵">1.1.1. 什么是矩阵</h4>
<div class="ulist">
<ul>
<li>
<p>矩阵（Matrix）是一个按照长方阵列排列的复数或实数集合</p>
</li>
<li>
<p>矩阵最早来自于方程组的系数及常数所构成的方阵，最初是用来解决线性方程求解的工具</p>
</li>
<li>
<p>矩阵是高等代数中的常见工具，也常见于统计分析等应用数学学科中；矩阵在物理学和计算机科学中都有应用</p>
</li>
<li>
<p>矩阵的运算是数值分析领域的重要问题</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵">1.1.2. 矩阵</h4>
<div class="sect4">
<h5 id="_矩阵的定义">矩阵的定义</h5>
<div class="stemblock">
<div class="content">
\[A =
\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} &amp; \dots  &amp; a_{1n} \\
    a_{21} &amp; a_{22} &amp; a_{23} &amp; \dots  &amp; a_{2n} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; a_{m3} &amp; \dots  &amp; a_{mn}
\end{bmatrix}\]
</div>
</div>
<div class="paragraph">
<p>以上是\(m \times n\)矩阵的定义。</p>
</div>
<div class="ulist">
<ul>
<li>
<p>由\(m \times n\)个数\(a_{ij}, (i = 1,2,...,m;  j = 1,2,...,n)\)排成的m行n列的数表 A就称为m行n列的矩阵。</p>
</li>
<li>
<p>这\(m \times n\)个数称作矩阵A的元素\(a_{ij}\)，元素位于矩阵A的第i行第j列。</p>
</li>
<li>
<p>\(m \times n\)矩阵A可以记作\(A_{m \times n}\)，其中m是行数，n是列数，m, n &gt; 0。</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_特殊矩阵">特殊矩阵</h5>
<div class="paragraph">
<p>对于\(A_{m \times n}\)，如果m=n，即矩阵的行数与列数相等，那么称A为方阵。例如：</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
    1 &amp; 2 &amp; 3 \\
    4 &amp; 5 &amp; 6 \\
    7 &amp; 8 &amp; 9
\end{bmatrix}\]
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵中的概念">1.1.3. 矩阵中的概念</h4>
<div class="ulist">
<ul>
<li>
<p>行数与列数都等于n的矩阵称为n阶矩阵，又称做n阶方阵，可以记作\(A_n\)。</p>
</li>
<li>
<p>只有一行的矩阵\(A_{1 \times n}\)称为行矩阵，又叫行向量。</p>
</li>
<li>
<p>同样，只有一列的矩阵\(A_{n \times 1}\)称为列矩阵，又叫列向量。</p>
</li>
<li>
<p>对于方阵，从左上角到右下角的直线，叫做主对角线，主对角线上的元素称为主对角线元素。</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
    \textbf{1} &amp; 2 &amp; 3 \\
    4 &amp; \textbf{5} &amp; 6 \\
    7 &amp; 8 &amp; \textbf{9}
\end{bmatrix}\]
</div>
</div>
<div class="sect4">
<h5 id="_特殊矩阵_2">特殊矩阵</h5>
<div class="ulist">
<ul>
<li>
<p>矩阵的元素全部为0，称为零矩阵，用O表示。</p>
</li>
<li>
<p>对于方阵，如果只有对角线元素为1，其余元素都为0，那么称为单位矩阵，一般用I或者E表示。</p>
</li>
<li>
<p>对于方阵，不在对角线上的元素都为0，称为对角矩阵。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵的加法">1.1.4. 矩阵的加法</h4>
<div class="ulist">
<ul>
<li>
<p>把矩阵的对应位元素相加。</p>
</li>
<li>
<p>矩阵的形状必须一致，即必须是同型矩阵。</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}
+
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{bmatrix}
=
\begin{bmatrix}
1+1 &amp; 2+1 \\
3+1 &amp; 4+1
\end{bmatrix}
=
\begin{bmatrix}
2 &amp; 3 \\
4 &amp; 5
\end{bmatrix}\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵的乘法">1.1.5. 矩阵的乘法</h4>
<div class="sect4">
<h5 id="_数与矩阵相乘">数与矩阵相乘</h5>
<div class="paragraph">
<p>数值与矩阵每一个元素相乘。</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}
\times
2
=
\begin{bmatrix}
1 \times 2 &amp; 2 \times 2 \\
3 \times 2 &amp; 4 \times 2
\end{bmatrix}
=
\begin{bmatrix}
2 &amp; 4 \\
6 &amp; 8
\end{bmatrix}\]
</div>
</div>
</div>
<div class="sect4">
<h5 id="_矩阵与矩阵相乘">矩阵与矩阵相乘</h5>
<div class="ulist">
<ul>
<li>
<p>左矩阵的每一行与右矩阵的每一列，对应每一个元素相乘。</p>
</li>
<li>
<p>\(A \times B\)，那么有A矩阵\(m \times n\)，B 矩阵\(n \times k\)，要求左侧矩阵的列数n，必须等于右侧矩阵的行数n，结果矩阵C为\(m \times k\)矩阵。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>定义：</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} \\
    a_{21} &amp; a_{22} &amp; a_{23}
\end{bmatrix}
\times
\begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{21} &amp; b_{22} \\
    b_{31} &amp; b_{32}
\end{bmatrix}
=
\begin{bmatrix}
    a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} &amp; a_{11}b_{12}+a_{12}b_{22}+a_{13}b_{32} \\
    a_{21}b_{11}+a_{22}b_{21}+a_{23}b_{31} &amp; a_{21}b_{12}+a_{22}b_{22}+a_{23}b_{32}
\end{bmatrix}\]
</div>
</div>
<div class="paragraph">
<p>练习：</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
    1 &amp; 2 &amp; 3 \\
    4 &amp; 5 &amp; 6 \\
    7 &amp; 8 &amp; 9
\end{bmatrix}
\times
\begin{bmatrix}
    1 &amp; 2 &amp; 3 \\
    4 &amp; 5 &amp; 6 \\
    7 &amp; 8 &amp; 9
\end{bmatrix}
=?\]
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵的转置">1.1.6. 矩阵的转置</h4>
<div class="ulist">
<ul>
<li>
<p>把矩阵A的行换成相同序数的列，得到一个新矩阵，叫做A的转置矩阵，记作\(A^T\)</p>
</li>
<li>
<p>行变列，列变行</p>
</li>
<li>
<p>A 为\(m \times n\)矩阵，转置之后为\(m \times n\)矩阵</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} \\
    a_{21} &amp; a_{22} &amp; a_{23}
\end{bmatrix}
\rightarrow
\begin{bmatrix}
    a_{11} &amp; a_{21} \\
    a_{12} &amp; a_{22} \\
    a_{13} &amp; a_{23}
\end{bmatrix}\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵的运算方法">1.1.7. 矩阵的运算方法</h4>
<div class="sect4">
<h5 id="_加法">加法</h5>
<div class="ulist">
<ul>
<li>
<p>\(A + B = B + A\)</p>
</li>
<li>
<p>\((A + B) + C = A + (B + C)\)</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_乘法">乘法</h5>
<div class="ulist">
<ul>
<li>
<p>\((λμ)A = λ(μA)\)</p>
</li>
<li>
<p>\((λ + μ)A = λA + μA\)</p>
</li>
<li>
<p>\(λ(A + B) = λA + λB\)</p>
</li>
<li>
<p>\((AB) C = A(BC)\)</p>
</li>
<li>
<p>\(λ(AB) = (λA)B = A(λB)\)</p>
</li>
<li>
<p>\(A(B + C) = AB + AC\)</p>
</li>
<li>
<p>\((B + C)A = BA + CA\)</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_减法">减法</h5>
<div class="ulist">
<ul>
<li>
<p>\(A - B = A + B × (-1)\)</p>
</li>
<li>
<p>\(A - A = A + (-A) = O\)</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_转置">转置</h5>
<div class="ulist">
<ul>
<li>
<p>\((A^T)^T = A\)</p>
</li>
<li>
<p>\((A + B)^T = A^T + B^T\)</p>
</li>
<li>
<p>\(λ(A)^T = λA^T\)</p>
</li>
<li>
<p>\((AB)^T = B^T A^T\)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵的逆">1.1.8. 矩阵的逆</h4>
<div class="ulist">
<ul>
<li>
<p>对于n阶方阵A，如果有一个n阶方阵B，使得\(AB = BA = E\)，就称矩阵A是可逆的，并把B称为A的逆矩阵。</p>
</li>
<li>
<p>A的逆矩阵记作\(A^{-1}\)，如果\(AB = BA = E\)，则\(B = A-1\)。</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{bmatrix}
  1 &amp; 2 &amp; 3 \\
  2 &amp; 2 &amp; 1 \\
  3 &amp; 4 &amp; 3
\end{bmatrix}
\times
\begin{bmatrix}
  1 &amp; 3 &amp; -2 \\
  -\frac{3}{2} &amp; -3 &amp; \frac{5}{2} \\
  1 &amp; 1 &amp; -1
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\]
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_微积分基础知识">1.2. 微积分基础知识</h3>
<div class="ulist">
<ul>
<li>
<p>什么是导数</p>
</li>
<li>
<p>偏导数</p>
</li>
<li>
<p>方向导数和梯度</p>
</li>
<li>
<p>凸函数和凹函数</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_什么是导数">1.2.1. 什么是导数</h4>
<div class="imageblock">
<div class="content">
<img src="images/derivative.png" alt="derivative">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>导数反映的是函数y = f(x)在某一点处沿x轴正方向的变化率</p>
</li>
<li>
<p>在x轴上某一点处，如果f'(x)&gt;0，说明f(x)的函数值在x点沿x轴正方向是趋于增加的；如果f'(x)&lt;0，说明f(x)的函数值在x点沿x轴正方向是趋于减少的</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_偏导数">1.2.2. 偏导数</h4>
<div class="imageblock">
<div class="content">
<img src="images/piandaoshu.png" alt="piandaoshu">
</div>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\partial}{\partial x_j} f(x_0,x_1,\dots,x_n)
=
\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x}
=
\lim_{\Delta x \to 0} \frac{f(x_0,\dots,x_j+\Delta x, \dots, x_n)-f(x_0,\dots,x_j,\dots,x_n)}{\Delta x}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限</p>
</li>
<li>
<p>偏导数也就是函数在某一点上沿某个坐标轴正方向的的变化率</p>
</li>
<li>
<p>导数指的是一元函数中，函数y=f(x)在某一点处沿x轴正方向的变化率；而偏导数，指的是多元函数中，函数\(y=f(x_1,x_2,…,x_n)\)在某一点处沿某一坐标轴\((x_1,x_2,…,x_n)\)正方向的变化率</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_方向导数">1.2.3. 方向导数</h4>
<div class="ulist">
<ul>
<li>
<p>函数某一点在某一趋近方向（向量方向）上的导数值</p>
</li>
<li>
<p>方向导数就是函数在除坐标轴正方向外，其他特定方向上的变化率</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/fangxiang.png" alt="fangxiang">
</div>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\partial}{\partial l} f(x_0, x_1, \dots, x_n)
=
\lim_{\rho \to 0} \frac{\Delta y}{\Delta x}
=
\lim_{\rho \to 0} \frac{f(x_0+\Delta x_0,\dots,x_j+\Delta x_j, \dots, x_n+\Delta x_n)-f(x_0,\dots,x_1,\dots,x_n)}{\rho} \\
其中，\rho=\sqrt{(\Delta x_0)^2+\dots+(\Delta x_j)^2+(\Delta x_n)^2} \\
l = (\Delta x_0, \Delta x_1, \dots, \Delta x_n)\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_梯度gradient">1.2.4. 梯度（Gradient）</h4>
<div class="paragraph">
<p>问题：函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？</p>
</div>
<div class="paragraph">
<p>答案：梯度。</p>
</div>
<div class="stemblock">
<div class="content">
\[gradf(x_0, x_1, \dots, x_n)=(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n})\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/tidu.png" alt="tidu">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>定义：函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值</p>
</li>
<li>
<p>梯度是一个向量，即有方向、有大小； </p>
</li>
<li>
<p>梯度的方向是最大方向导数的方向；梯度的值是最大方向导数的值</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_凸函数和凹函数">1.2.5. 凸函数和凹函数</h4>
<div class="paragraph">
<p><span class="image"><img src="images/tuao.png" alt="tuao"></span></p>
</div>
<div class="sect4">
<h5 id="_凸函数">凸函数</h5>
<div class="paragraph">
<p>凸函数是具有如下特性的一个定义在某个向量空间的凸子集C（区间）上的实值函数f：对其定义域C上的任意两点\(x_{1},x_{2}\)，总有\(f(\frac{x_1+x_2}{2}) \le \frac{f(x_1)+f(x_2)}{2}\)。</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/tuhanshu.png" alt="tuhanshu">
</div>
</div>
</div>
<div class="sect4">
<h5 id="_凹函数">凹函数</h5>
<div class="paragraph">
<p>我们称一个有实值函数f在某区间（或者某个向量空间中的凹集）上是凹的，如果对任意该区间内不相等的x和y和[0,1]中的任意t有</p>
</div>
<div class="stemblock">
<div class="content">
\[f(tx+(1-t)y) \ge tf(x)+(1-t)f(y)\]
</div>
</div>
<div class="paragraph">
<p>某函数f:R→R，在x和y之间的每一点z，在图中的点(z, f(z))是在以点(x, f(x)) and (y, f(y))连成的直线之上。</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/aohanshu.png" alt="aohanshu">
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_概率论和统计学">1.3. 概率论和统计学</h3>
<div class="ulist">
<ul>
<li>
<p>常用统计变量</p>
</li>
<li>
<p>常见概率分布</p>
</li>
<li>
<p>重要概率公式</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_常用统计变量">1.3.1. 常用统计变量</h4>
<div class="ulist">
<ul>
<li>
<p>样本均值</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[E(X)=\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>样本方差</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[D(X)=S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2
=\frac{1}{n-1}\sum_{i=1}^n (X^2_i-n \bar{X})\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>样本标准差</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\sqrt{D(X)}=S=\sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2}\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_常见概率分布">1.3.2. 常见概率分布</h4>
<div class="ulist">
<ul>
<li>
<p>均匀分布</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[f(x)=\frac{1}{b-a}, a &lt; x &lt; b \\
f(x)=0, else\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/junyunfenbu.png" alt="junyunfenbu">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>正态分布（高斯分布）</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[f(x)=\frac{1}{\sqrt {2 \pi} \sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/zhengtai.png" alt="zhengtai">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>指数分布</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{equation}
f(x)
=
\begin{cases}
    \lambda e^{- \lambda x} &amp; x \ge 0 \\
    0 &amp; x &lt; 0
\end{cases}
\end{equation}\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/zhishu.png" alt="zhishu">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_重要概率公式">1.3.3. 重要概率公式</h4>
<div class="ulist">
<ul>
<li>
<p>条件概率公式</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[P(B \vert A)=\frac{P(AB)}{P(A)}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>全概率公式</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[P(A) = P(A \vert B_1)P(B_1) + P(A \vert B_2)P(B_2) + \dots + P(A \vert B_n)P(B_n)\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>贝叶斯公式</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[P(B_i \vert A) = \frac{P(A \vert B_i)P(B_i)}{\sum_{j=1}^{n}P(A \vert B_j)P(B_j)}\]
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_数学基础精简进阶版">2. 数学基础(精简进阶版)</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_线性代数_2">2.1. 线性代数</h3>
<div class="paragraph">
<p>下面分别概括了向量、矩阵、运算、范数、特征向量和特征值的概念。</p>
</div>
<div class="sect3">
<h4 id="_向量">2.1.1. 向量</h4>
<div class="paragraph">
<p>本书中的向量指的是列向量。一个\(n\)维向量\(\boldsymbol{x}\)的表达式可写成</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{x} =
\begin{bmatrix}
    x_{1}  \\
    x_{2}  \\
    \vdots  \\
    x_{n}
\end{bmatrix},\]
</div>
</div>
<div class="paragraph">
<p>其中\(x_1, \ldots, x_n\)是向量的元素。我们将各元素均为实数的\(n\)维向量\(\boldsymbol{x}\)记作\(\boldsymbol{x} \in \mathbb{R}^{n}\)或\(\boldsymbol{x} \in \mathbb{R}^{n \times 1}\)。</p>
</div>
</div>
<div class="sect3">
<h4 id="_矩阵_2">2.1.2. 矩阵</h4>
<div class="paragraph">
<p>一个\(m\)行\(n\)列矩阵的表达式可写成</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{X} =
\begin{bmatrix}
    x_{11} &amp; x_{12}  &amp; \dots  &amp; x_{1n} \\
    x_{21} &amp; x_{22}  &amp; \dots  &amp; x_{2n} \\
    \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\
    x_{m1} &amp; x_{m2}  &amp; \dots  &amp; x_{mn}
\end{bmatrix},\]
</div>
</div>
<div class="paragraph">
<p>其中\(x_{ij}\)是矩阵\(\boldsymbol{X}\)中第\(i\)行第\(j\)列的元素（\(1 \leq i \leq m, 1 \leq j \leq n\)）。我们将各元素均为实数的\(m\)行\(n\)列矩阵\(\boldsymbol{X}\)记作\(\boldsymbol{X} \in \mathbb{R}^{m \times n}\)。不难发现，向量是特殊的矩阵。</p>
</div>
</div>
<div class="sect3">
<h4 id="_运算">2.1.3. 运算</h4>
<div class="paragraph">
<p>设\(n\)维向量\(\boldsymbol{A}\)中的元素为\(a_1, \ldots, a_n\)，\(n\)维向量\(\boldsymbol{b}\)中的元素为\(b_1, \ldots, b_n\)。向量\(\boldsymbol{A}\)与\(\boldsymbol{b}\)的点乘（内积）是一个标量：</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{a} \cdot \boldsymbol{b} = a_1 b_1 + \ldots + a_n b_n.\]
</div>
</div>
<div class="paragraph">
<p>设两个\(m\)行\(n\)列矩阵</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A} =
\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; \dots  &amp; a_{1n} \\
    a_{21} &amp; a_{22} &amp; \dots  &amp; a_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; \dots  &amp; a_{mn}
\end{bmatrix},\quad
\boldsymbol{B} =
\begin{bmatrix}
    b_{11} &amp; b_{12} &amp; \dots  &amp; b_{1n} \\
    b_{21} &amp; b_{22} &amp; \dots  &amp; b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    b_{m1} &amp; b_{m2} &amp; \dots  &amp; b_{mn}
\end{bmatrix}.\]
</div>
</div>
<div class="paragraph">
<p>矩阵\(\boldsymbol{A}\)的转置是一个\(n\)行\(m\)列矩阵，它的每一行其实是原矩阵的每一列：</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A}^\top =
\begin{bmatrix}
    a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \\
    a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn}
\end{bmatrix}.\]
</div>
</div>
<div class="paragraph">
<p>两个相同形状的矩阵的加法是将两个矩阵按元素做加法：</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A} + \boldsymbol{B} =
\begin{bmatrix}
    a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \dots  &amp; a_{1n} + b_{1n} \\
    a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \dots  &amp; a_{2n} + b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \dots  &amp; a_{mn} + b_{mn}
\end{bmatrix}.\]
</div>
</div>
<div class="paragraph">
<p>我们使用符号\(\odot\)表示两个矩阵按元素做乘法的运算：</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A} \odot \boldsymbol{B} =
\begin{bmatrix}
    a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \\
    a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn}
\end{bmatrix}.\]
</div>
</div>
<div class="paragraph">
<p>定义一个标量\(k\)。标量与矩阵的乘法也是按元素做乘法的运算：</p>
</div>
<div class="stemblock">
<div class="content">
\[k\boldsymbol{A} =
\begin{bmatrix}
    ka_{11} &amp; ka_{12} &amp; \dots  &amp; ka_{1n} \\
    ka_{21} &amp; ka_{22} &amp; \dots  &amp; ka_{2n} \\
    \vdots &amp; \vdots   &amp; \ddots &amp; \vdots \\
    ka_{m1} &amp; ka_{m2} &amp; \dots  &amp; ka_{mn}
\end{bmatrix}.\]
</div>
</div>
<div class="paragraph">
<p>其他诸如标量与矩阵按元素相加、相除等运算与上式中的相乘运算类似。矩阵按元素开根号、取对数等运算也就是对矩阵每个元素开根号、取对数等，并得到和原矩阵形状相同的矩阵。</p>
</div>
<div class="paragraph">
<p>矩阵乘法和按元素的乘法不同。设\(\boldsymbol{A}\)为\(m\)行\(p\)列的矩阵，\(\boldsymbol{B}\)为\(p\)行\(n\)列的矩阵。两个矩阵相乘的结果</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A} \boldsymbol{B} =
\begin{bmatrix}
    a_{11} &amp; a_{12} &amp; \dots  &amp; a_{1p} \\
    a_{21} &amp; a_{22} &amp; \dots  &amp; a_{2p} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{i1} &amp; a_{i2} &amp; \dots  &amp; a_{ip} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1} &amp; a_{m2} &amp; \dots  &amp; a_{mp}
\end{bmatrix}
\begin{bmatrix}
    b_{11} &amp; b_{12} &amp; \dots  &amp; b_{1j} &amp; \dots &amp; b_{1n} \\
    b_{21} &amp; b_{22} &amp; \dots  &amp; b_{2j} &amp; \dots  &amp; b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots \\
    b_{p1} &amp; b_{p2} &amp; \dots  &amp; b_{pj} &amp; \dots  &amp; b_{pn}
\end{bmatrix}\]
</div>
</div>
<div class="paragraph">
<p>是一个\(m\)行\(n\)列的矩阵，其中第\(i\)行第\(j\)列（\(1 \leq i \leq m, 1 \leq j \leq n\)）的元素为</p>
</div>
<div class="stemblock">
<div class="content">
\[a_{i1}b_{1j}  + a_{i2}b_{2j} + \ldots + a_{ip}b_{pj} = \sum_{k=1}^p a_{ik}b_{kj}.\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_范数">2.1.4. 范数</h4>
<div class="paragraph">
<p>设\(n\)维向量\(\boldsymbol{x}\)中的元素为\(x_1, \ldots, x_n\)。向量\(\boldsymbol{x}\)的\(L_p\)范数为</p>
</div>
<div class="stemblock">
<div class="content">
\[\|\boldsymbol{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.\]
</div>
</div>
<div class="paragraph">
<p>例如，\(\boldsymbol{x}\)的\(L_1\)范数是该向量元素绝对值之和：</p>
</div>
<div class="stemblock">
<div class="content">
\[\|\boldsymbol{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.\]
</div>
</div>
<div class="paragraph">
<p>而\(\boldsymbol{x}\)的\(L_2\)范数是该向量元素平方和的平方根：</p>
</div>
<div class="stemblock">
<div class="content">
\[\|\boldsymbol{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}.\]
</div>
</div>
<div class="paragraph">
<p>我们通常用\(\|\boldsymbol{x}\|\)指代\(\|\boldsymbol{x}\|_2\)。</p>
</div>
<div class="paragraph">
<p>设\(\boldsymbol{x}\)是一个\(m\)行\(n\)列矩阵。矩阵\(\boldsymbol{x}\)的Frobenius范数为该矩阵元素平方和的平方根：</p>
</div>
<div class="stemblock">
<div class="content">
\[\|\boldsymbol{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2},\]
</div>
</div>
<div class="paragraph">
<p>其中\(x_{ij}\)为矩阵\(\boldsymbol{x}\)在第\(i\)行第\(j\)列的元素。</p>
</div>
</div>
<div class="sect3">
<h4 id="_特征向量和特征值">2.1.5. 特征向量和特征值</h4>
<div class="paragraph">
<p>对于一个\(n\)行\(n\)列的矩阵\(\boldsymbol{A}\)，假设有标量\(\lambda\)和非零的\(n\)维向量\(\boldsymbol{v}\)使</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v},\]
</div>
</div>
<div class="paragraph">
<p>那么\(\boldsymbol{v}\)是矩阵\(\boldsymbol{A}\)的一个特征向量，标量\(\lambda\)是\(\boldsymbol{v}\)对应的特征值。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_微分">2.2. 微分</h3>
<div class="paragraph">
<p>我们在这里简要介绍微分的一些基本概念和演算。</p>
</div>
<div class="sect3">
<h4 id="_导数和微分">2.2.1. 导数和微分</h4>
<div class="paragraph">
<p>假设函数\(f: \mathbb{R} \rightarrow \mathbb{R}\)的输入和输出都是标量。函数\(f\)的导数</p>
</div>
<div class="stemblock">
<div class="content">
\[f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h},\]
</div>
</div>
<div class="paragraph">
<p>且假定该极限存在。给定\(y = f(x)\)，其中\(x\)和\(y\)分别是函数\(f\)的自变量和因变量。以下有关导数和微分的表达式等价：</p>
</div>
<div class="stemblock">
<div class="content">
\[f'(x) = y' = \frac{\text{d}y}{\text{d}x} = \frac{\text{d}f}{\text{d}x} = \frac{\text{d}}{\text{d}x} f(x) = \text{D}f(x) = \text{D}_x f(x),\]
</div>
</div>
<div class="paragraph">
<p>其中符号\(\text{D}\)和\(\text{d}/\text{d}x\)也叫微分运算符。常见的微分演算有\(\text{D}C = 0\)（\(C\)为常数）、\(\text{D}x^n = nx^{n-1}\)（\(n\)为常数）、\(\text{D}e^x = e^x\)、\(\text{D}\ln(x) = 1/x\)等。</p>
</div>
<div class="paragraph">
<p>如果函数\(f\)和\(g\)都可导，设\(C\)为常数，那么</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{aligned}
\frac{\text{d}}{\text{d}x} [Cf(x)] &amp;= C \frac{\text{d}}{\text{d}x} f(x),\\
\frac{\text{d}}{\text{d}x} [f(x) + g(x)] &amp;= \frac{\text{d}}{\text{d}x} f(x) + \frac{\text{d}}{\text{d}x} g(x),\\
\frac{\text{d}}{\text{d}x} [f(x)g(x)] &amp;= f(x) \frac{\text{d}}{\text{d}x} [g(x)] + g(x) \frac{\text{d}}{\text{d}x} [f(x)],\\
\frac{\text{d}}{\text{d}x} \left[\frac{f(x)}{g(x)}\right] &amp;= \frac{g(x) \frac{\text{d}}{\text{d}x} [f(x)] - f(x) \frac{\text{d}}{\text{d}x} [g(x)]}{[g(x)]^2}.
\end{aligned}\]
</div>
</div>
<div class="paragraph">
<p>如果\(y=f(u)\)和\(u=g(x)\)都是可导函数，依据链式法则，</p>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\text{d}y}{\text{d}x} = \frac{\text{d}y}{\text{d}u} \frac{\text{d}u}{\text{d}x}.\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_泰勒展开">2.2.2. 泰勒展开</h4>
<div class="paragraph">
<p>函数\(f\)的泰勒展开式是</p>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n,\]
</div>
</div>
<div class="paragraph">
<p>其中\(f^{(n)}\)为函数\(f\)的\(n\)阶导数（求\(n\)次导数），\(n!\)为\(n\)的阶乘。假设\(\epsilon\)是一个足够小的数，如果将上式中\(x\)和\(a\)分别替换成\(x+\epsilon\)和\(x\)，可以得到</p>
</div>
<div class="stemblock">
<div class="content">
\[f(x + \epsilon) \approx f(x) + f'(x) \epsilon + \mathcal{O}(\epsilon^2).\]
</div>
</div>
<div class="paragraph">
<p>由于\(\epsilon\)足够小，上式也可以简化成</p>
</div>
<div class="stemblock">
<div class="content">
\[f(x + \epsilon) \approx f(x) + f'(x) \epsilon.\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_偏导数_2">2.2.3. 偏导数</h4>
<div class="paragraph">
<p>设\(u\)为一个有\(n\)个自变量的函数，\(u = f(x_1, x_2, \ldots, x_n)\)，它有关第\(i\)个变量\(x_i\)的偏导数为</p>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\partial u}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.\]
</div>
</div>
<div class="paragraph">
<p>以下有关偏导数的表达式等价：</p>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\partial u}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = \text{D}_i f = \text{D}_{x_i} f.\]
</div>
</div>
<div class="paragraph">
<p>为了计算\(\partial u/\partial x_i\)，只需将\(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n\)视为常数并求\(u\)有关\(x_i\)的导数。</p>
</div>
</div>
<div class="sect3">
<h4 id="_梯度">2.2.4. 梯度</h4>
<div class="paragraph">
<p>假设函数\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)的输入是一个\(n\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_n\)^\top]，输出是标量。函数\(f(\boldsymbol{x})\)有关\(\boldsymbol{x}\)的梯度是一个由\(n\)个偏导数组成的向量：</p>
</div>
<div class="stemblock">
<div class="content">
\[\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n}\bigg]^\top.\]
</div>
</div>
<div class="paragraph">
<p>为表示简洁，我们有时用\(\nabla f(\boldsymbol{x})\)代替\(\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\)。</p>
</div>
<div class="paragraph">
<p>假设\(\boldsymbol{x}\)是一个向量，常见的梯度演算包括</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{aligned}
\nabla_{\boldsymbol{x}} \boldsymbol{A}^\top \boldsymbol{x} &amp;= \boldsymbol{A}, \\
\nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{A}  &amp;= \boldsymbol{A}, \\
\nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x}  &amp;= (\boldsymbol{A} + \boldsymbol{A}^\top)\boldsymbol{x},\\
\nabla_{\boldsymbol{x}} \|\boldsymbol{x} \|^2 &amp;= \nabla_{\boldsymbol{x}} \boldsymbol{x}^\top \boldsymbol{x} = 2\boldsymbol{x}.
\end{aligned}\]
</div>
</div>
<div class="paragraph">
<p>类似地，假设\(\boldsymbol{x}\)是一个矩阵，那么</p>
</div>
<div class="stemblock">
<div class="content">
\[\nabla_{\boldsymbol{X}} \|\boldsymbol{X} \|_F^2 = 2\boldsymbol{X}.\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_海森矩阵">2.2.5. 海森矩阵</h4>
<div class="paragraph">
<p>假设函数\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)的输入是一个\(n\)维向量\(\boldsymbol{x} = [x_1, x_2, \ldots, x_n\)^\top]，输出是标量。假定函数\(f\)所有的二阶偏导数都存在，\(f\)的海森矩阵\(\boldsymbol{H}\)是一个\(n\)行\(n\)列的矩阵：</p>
</div>
<div class="stemblock">
<div class="content">
\[\boldsymbol{H} =
\begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix},\]
</div>
</div>
<div class="paragraph">
<p>其中二阶偏导数</p>
</div>
<div class="stemblock">
<div class="content">
\[\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial }{\partial x_j} \left(\frac{\partial f}{ \partial x_i}\right).\]
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_概率">2.3. 概率</h3>
<div class="paragraph">
<p>最后，我们简要介绍条件概率、期望和均匀分布。</p>
</div>
<div class="sect3">
<h4 id="_条件概率">2.3.1. 条件概率</h4>
<div class="paragraph">
<p>假设事件\(A\)和事件\(B\)的概率分别为\(P(A)\)和\(P(B)\)，两个事件同时发生的概率记作\(P(A \cap B)\)或\(P(A, B)\)。给定事件\(B\)，事件\(A\)的条件概率</p>
</div>
<div class="stemblock">
<div class="content">
\[P(A \mid B) = \frac{P(A \cap B)}{P(B)}.\]
</div>
</div>
<div class="paragraph">
<p>也就是说，</p>
</div>
<div class="stemblock">
<div class="content">
\[P(A \cap B) = P(B) P(A \mid B) = P(A) P(B \mid A).\]
</div>
</div>
<div class="paragraph">
<p>当满足</p>
</div>
<div class="stemblock">
<div class="content">
\[P(A \cap B) = P(A) P(B)\]
</div>
</div>
<div class="paragraph">
<p>时，事件\(A\)和事件\(B\)相互独立。</p>
</div>
</div>
<div class="sect3">
<h4 id="_期望">2.3.2. 期望</h4>
<div class="paragraph">
<p>离散的随机变量\(X\)的期望（或平均值）为</p>
</div>
<div class="stemblock">
<div class="content">
\[E(X) = \sum_{x} x P(X = x).\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_均匀分布">2.3.3. 均匀分布</h4>
<div class="paragraph">
<p>假设随机变量\(X\)服从\([a, b\)]上的均匀分布，即\(X \sim U(a, b)\)。随机变量\(X\)取\(a\)和\(b\)之间任意一个数的概率相等。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_机器学习简介">3. 机器学习简介</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>机器学习的概念</p>
</li>
<li>
<p>机器学习主要分类</p>
</li>
<li>
<p>监督学习三要素</p>
</li>
<li>
<p>监督学习模型评估策略</p>
</li>
<li>
<p>监督学习模型求解算法</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_机器学习的概念">3.1. 机器学习的概念</h3>
<div class="ulist">
<ul>
<li>
<p>机器学习是什么</p>
</li>
<li>
<p>机器学习的开端</p>
</li>
<li>
<p>机器学习的定义</p>
</li>
<li>
<p>机器学习的过程</p>
</li>
<li>
<p>机器学习示例</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_机器学习是什么">3.1.1. 机器学习是什么</h4>
<div class="ulist">
<ul>
<li>
<p>什么是学习</p>
<div class="ulist">
<ul>
<li>
<p>从人的学习说起</p>
</li>
<li>
<p>学习理论；从实践经验中总结</p>
</li>
<li>
<p>在理论上推导；在实践中检验</p>
</li>
<li>
<p>通过各种手段获取知识或技能的过程</p>
</li>
</ul>
</div>
</li>
<li>
<p>机器怎么学习？</p>
<div class="ulist">
<ul>
<li>
<p>处理某个特定的任务，以大量的“经验”为基础</p>
</li>
<li>
<p>对任务完成的好坏，给予一定的评判标准</p>
</li>
<li>
<p>通过分析经验数据，任务完成得更好了</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_机器学习的开端">3.1.2. 机器学习的开端</h4>
<div class="ulist">
<ul>
<li>
<p>1952年，IBM的Arthur Samuel（被誉为“机器学习之父”）设计了一款可以学习的西洋跳棋程序。</p>
</li>
<li>
<p>它能通过观察棋子的走位来构建新的模型，并用其提高自己的下棋技巧。</p>
</li>
<li>
<p>Samuel和这个程序进行多场对弈后发现，随着时间的推移，程序的棋艺变得越来越好。</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_机器学习的定义">3.1.3. 机器学习的定义</h4>
<div class="ulist">
<ul>
<li>
<p>机器学习(Machine Learning, ML)主要研究计算机系统对于特定任务的性能，逐步进行改善的算法和统计模型。</p>
</li>
<li>
<p>通过输入海量训练数据对模型进行训练，使模型掌握数据所蕴含的潜在规律，进而对新输入的数据进行准确的分类或预测。</p>
</li>
<li>
<p>是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸优化、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_机器学习的过程">3.1.4. 机器学习的过程</h4>
<div class="paragraph">
<p>海量数据 &#8594; 提炼规律 &#8594; 预测未来</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_机器学习的分类">3.2. 机器学习的分类</h3>
<div class="ulist">
<ul>
<li>
<p>监督学习：提供数据并提供数据对应结果的机器学习过程。</p>
</li>
<li>
<p>无监督学习：提供数据并且不提供数据对应结果的机器学习过程。</p>
</li>
<li>
<p>强化学习：通过与环境交互并获取延迟返回进而改进行为的学习过程。</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/class.png" alt="class">
</div>
</div>
<div class="sect3">
<h4 id="_监督学习">3.2.1. 监督学习</h4>
<div class="imageblock">
<div class="content">
<img src="images/supervised.png" alt="supervised">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>监督学习（Supervised Learning）算法构建了包含输入和所需输出的一组数据的数学模型。这些数据称为训练数据，由一组训练样本组成。</p>
</li>
<li>
<p>监督学习主要包括分类和回归。</p>
</li>
<li>
<p>当输出被限制为有限的一组值（离散数值）时使用分类算法；当输出可以具有范围内的任何数值（连续数值）时使用回归算法。</p>
</li>
<li>
<p>相似度学习是和回归和分类都密切相关的一类监督机器学习，它的目标是使用相似性函数从样本中学习，这个函数可以度量两个对象之间的相似度或关联度。它在排名、推荐系统、视觉识别跟踪、人脸识别等方面有很好的应用场景。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>例子</strong></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. 预测房价或者房屋出售情况</caption>
<colgroup>
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">所在街区</th>
<th class="tableblock halign-left valign-top">房屋价格</th>
<th class="tableblock halign-left valign-top">住房面积</th>
<th class="tableblock halign-left valign-top">住房格局</th>
<th class="tableblock halign-left valign-top">是否学区</th>
<th class="tableblock halign-left valign-top">是否售出</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">海淀</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">7000000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">120</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">三室一厅</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">是</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">是</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">朝阳</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6000000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">二室一厅</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">否</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">否</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">昌平</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5000000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">120</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">二室一厅</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">否</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">是</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">大兴</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6500000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">150</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">三室一厅</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">否</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">？</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_监督学习深入介绍">3.2.2. 监督学习深入介绍</h4>
<div class="ulist">
<ul>
<li>
<p>监督学习三要素</p>
</li>
<li>
<p>监督学习实现步骤</p>
</li>
<li>
<p>监督学习模型评估策略</p>
</li>
<li>
<p>分类和回归</p>
</li>
<li>
<p>监督学习模型求解算法</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_监督学习三要素">监督学习三要素</h5>
<div class="ulist">
<ul>
<li>
<p>模型（model）：总结数据的内在规律，用数学函数描述的系统</p>
</li>
<li>
<p>策略（strategy）：选取最优模型的评价准则</p>
</li>
<li>
<p>算法（algorithm）：选取最优模型的具体方法</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_监督学习实现步骤">监督学习实现步骤</h5>
<div class="ulist">
<ul>
<li>
<p>得到一个有限的训练数据集</p>
</li>
<li>
<p>确定包含所有学习模型的集合</p>
</li>
<li>
<p>确定模型选择的准则，也就是学习策略</p>
</li>
<li>
<p>实现求解最优模型的算法，也就是学习算法</p>
</li>
<li>
<p>通过学习算法选择最优模型</p>
</li>
<li>
<p>利用得到的最优模型，对新数据进行预测或分析</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_监督学习过程示例">监督学习过程示例</h5>
<div class="paragraph">
<p>假设我们有一个如下的二元一次方程：\(Ax + B\)
我们已知两组数据：
* \(x = 1\)时，\(y = 3\)，即\((1, 3)\)
* \(x = 2\)时，\(y = 5\)，即\((2, 5)\)
将数据输入方程中，可得：</p>
</div>
<div class="stemblock">
<div class="content">
\[A + B = 3 \\
2A + B = 5\]
</div>
</div>
<div class="paragraph">
<p>解得：\(A = 2, B = 1\)
即方程为：\(2x + 1 = y\)
当我们有任意一个x时，输入方程，就可以得到对应的y
例如x = 5时，y = 11。</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/model1.png" alt="model1">
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_模型评估策略">3.2.3. 模型评估策略</h4>
<div class="ulist">
<ul>
<li>
<p>模型评估</p>
<div class="ulist">
<ul>
<li>
<p>训练集和测试集</p>
</li>
<li>
<p>损失函数和经验风险</p>
</li>
<li>
<p>训练误差和测试误差</p>
</li>
</ul>
</div>
</li>
<li>
<p>模型选择</p>
<div class="ulist">
<ul>
<li>
<p>过拟合和欠拟合</p>
</li>
<li>
<p>正则化和交叉验证</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_训练集和测试集">训练集和测试集</h5>
<div class="ulist">
<ul>
<li>
<p>我们将数据输入到模型中训练出了对应模型，但是模型的效果好不好呢？我们需要对模型的好坏进行评估</p>
</li>
<li>
<p>我们将用来训练模型的数据称为训练集，将用来测试模型好坏的集合称为测试集。</p>
</li>
<li>
<p>训练集：输入到模型中对模型进行训练的数据集合。</p>
</li>
<li>
<p>测试集：模型训练完成后测试训练效果的数据集合。</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_损失函数">损失函数</h5>
<div class="ulist">
<ul>
<li>
<p>损失函数用来衡量模型预测误差的大小。</p>
</li>
<li>
<p>定义：选取模型f为决策函数，对于给定的输入参数X，f(X)为预测结果，Y为真实结果；f(X)和Y之间可能会有偏差，我们就用一个损失函数（loss function）来度量预测偏差的程度，记作L(Y,f(X))</p>
</li>
<li>
<p>损失函数是系数的函数</p>
</li>
<li>
<p>损失函数值越小，模型就越好</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_常见损失函数">常见损失函数</h5>
<div class="ulist">
<ul>
<li>
<p>0-1损失函数</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{equation}
  L(Y,f(X)) =
    \begin{cases}
      1 &amp; Y \neq f(X) \\
      0 &amp; Y=f(X)
    \end{cases}
\end{equation}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>平方损失函数</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[L(Y,f(X))=(Y-f(X))^2\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>绝对损失函数</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[L(Y,f(X))=\vert Y-f(X) \vert\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>对数损失函数</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[L(Y,P(Y \vert X))=-logP(Y \vert X)\]
</div>
</div>
</div>
<div class="sect4">
<h5 id="_经验风险">经验风险</h5>
<div class="ulist">
<ul>
<li>
<p>经验风险</p>
<div class="ulist">
<ul>
<li>
<p>模型f(X)关于训练数据集的平均损失称为经验风险（empirial risk），记作\(R_{emp}\)</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[R_{emp}(f)=\frac{1}{N}\sum_{i=1}^N L(y_i, f(x_i))\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>经验风险最小化（Empirical Risk Minimization，ERM）</p>
<div class="ulist">
<ul>
<li>
<p>这一策略认为，经验风险最小的模型就是最优的模型</p>
</li>
<li>
<p>样本足够大时，ERM有很好的学习效果，因为有足够多的“经验”</p>
</li>
<li>
<p>样本较小时，ERM就会出现一些问题</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_训练误差和测试误差">训练误差和测试误差</h5>
<div class="ulist">
<ul>
<li>
<p>训练误差</p>
<div class="ulist">
<ul>
<li>
<p>训练误差（training error）是关于训练集的平均损失。</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[R_{emp}(\hat{f})=\frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i))\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>训练误差的大小，可以用来判断给定问题是否容易学习，但本质上并不重要</p>
<div class="ulist">
<ul>
<li>
<p>测试误差</p>
</li>
</ul>
</div>
</li>
<li>
<p>测试误差（testing error）是关于测试集的平均损失。</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[e_{test}(f)=\frac{1}{N'}\sum_{i=1}^{N'} L(y_i, \hat{f}(x_i))\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>测试误差真正反映了模型对未知数据的预测能力，这种能力一般被称为泛化能力。</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_过拟合和欠拟合">过拟合和欠拟合</h5>
<div class="imageblock">
<div class="content">
<img src="images/overfit.png" alt="overfit">
</div>
</div>
<div class="paragraph">
<p><strong>欠拟合</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>模型没有很好地捕捉到数据特征，特征集过小，导致模型不能很好地拟合数据，称之为欠拟合（under-fitting）</p>
</li>
<li>
<p>欠拟合的本质是对数据的特征“学习”得不够</p>
</li>
<li>
<p>例如，想分辨一只猫，只给出了四条腿、两只眼、有尾巴这三个特征，那么由此训练出来的模型根本无法分辨猫</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>过拟合</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>把训练数据学习的太彻底，以至于把噪声数据的特征也学习到了，特征集过大，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差，称之为过拟合（over-fitting）。</p>
</li>
<li>
<p>例如，想分辨一只猫，给出了四条腿、两只眼、一条尾巴、叫声、颜色，能够捕捉老鼠、喜欢吃鱼、&#8230;&#8203;，然后恰好所有的训练数据的猫都是白色，那么这个白色是一个噪声数据，会干扰判断，结果模型把颜色是白色也学习到了，而白色是局部样本的特征，不是全局特征，就造成了输入一个黑猫的数据，判断出不是猫。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>例子</strong></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/fitting.png" alt="fitting">
</div>
</div>
<div class="stemblock">
<div class="content">
\[f_M(x,w)=w_0+w_1x+w_2x^2+ \dots + w_Mx^M=\sum_{j=0}^Mw_jx^j\]
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_模型的选择">3.2.4. 模型的选择</h4>
<div class="ulist">
<ul>
<li>
<p>当模型复杂度增大时，训练误差会逐渐减小并趋向于0；而测试误差会先减小，达到最小值之后再增大</p>
</li>
<li>
<p>当模型复杂度过大时，就会发生过拟合；所以模型复杂度应适当</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/model.png" alt="model">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_正则化">3.2.5. 正则化</h4>
<div class="ulist">
<ul>
<li>
<p>结构风险最小化（Structural Risk Minimization，SRM）</p>
<div class="ulist">
<ul>
<li>
<p>是在 ERM 基础上，为了防止过拟合而提出来的策略</p>
</li>
<li>
<p>在经验风险上加上表示模型复杂度的正则化项（regularizer），或者叫惩罚项</p>
</li>
<li>
<p>正则化项一般是模型复杂度的单调递增函数，即模型越复杂，正则化值越大</p>
</li>
</ul>
</div>
</li>
<li>
<p>结构风险最小化的典型实现是正则化（regularization）</p>
<div class="ulist">
<ul>
<li>
<p>形式：</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\min_{f \in F} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>第一项是经验风险，第二项\(J(f)\)是正则化项，\(\lambda \geq 0\)是调整两者关系的系数</p>
</li>
<li>
<p>正则化项可以取不同的形式，比如，特征向量的\(L_1\)范数或\(L_2\)范数</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_奥卡姆剃刀">奥卡姆剃刀</h5>
<div class="ulist">
<ul>
<li>
<p>奥卡姆剃刀(Occam‘s razor)原理：如无必要，勿增实体</p>
</li>
<li>
<p>正则化符合奥卡姆剃刀原理。它的思想是：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型</p>
</li>
<li>
<p>如果简单的模型已经够用，我们不应该一味地追求更小的训练误差，而把模型变得越来越复杂</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_交叉验证">3.2.6. 交叉验证</h4>
<div class="ulist">
<ul>
<li>
<p>数据集划分</p>
<div class="ulist">
<ul>
<li>
<p>如果样本数据充足，一种简单方法是随机将数据集切成三部分：训练集（training set）、验证集（validation set）和测试集（test set）</p>
</li>
<li>
<p>训练集用于训练模型，验证集用于模型选择，测试集用于学习方法评估</p>
</li>
</ul>
</div>
</li>
<li>
<p>数据不充足时，可以重复地利用数据——交叉验证（cross validation）</p>
<div class="ulist">
<ul>
<li>
<p>简单交叉验证</p>
<div class="ulist">
<ul>
<li>
<p>数据随机分为两部分，如70%作为训练集，剩下30%作为测试集</p>
</li>
<li>
<p>训练集在不同的条件下（比如参数个数）训练模型，得到不同的模型</p>
</li>
<li>
<p>在测试集上评价各个模型的测试误差，选出最优模型</p>
</li>
</ul>
</div>
</li>
<li>
<p>S折交叉验证</p>
<div class="ulist">
<ul>
<li>
<p>将数据随机切分为S个互不相交、相同大小的子集；S-1个做训练集，剩下一个做测试集</p>
</li>
<li>
<p>重复进行训练集、测试集的选取，有S种可能的选择</p>
</li>
</ul>
</div>
</li>
<li>
<p>留一交叉验证</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_分类和回归">3.2.7. 分类和回归</h4>
<div class="ulist">
<ul>
<li>
<p>监督学习问题主要可以划分为两类，即分类问题和回归问题</p>
<div class="ulist">
<ul>
<li>
<p>分类问题预测数据属于哪一类别。 —— 离散</p>
</li>
<li>
<p>回归问题根据数据预测一个数值。 —— 连续</p>
</li>
</ul>
</div>
</li>
<li>
<p>通俗地讲，分类问题就是预测数据属于哪一种类型，就像上面的房屋出售预测，通过大量数据训练模型，然后去预测某个给定房屋能不能出售出去，属于能够出售类型还是不能出售类型。</p>
</li>
<li>
<p>回归问题就是预测一个数值，比如给出房屋一些特征，预测房价</p>
</li>
<li>
<p>如果将上面的房屋出售的问题改为预测房屋出售的概率，得到的结果将不再是可以售出（1）和不能售出（0），将会是一个连续的数值，例如 0.5，这就变成了一个回归问题</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_分类问题">分类问题</h5>
<div class="imageblock">
<div class="content">
<img src="images/fenlei.png" alt="fenlei">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>在监督学习中，当输出变量 Y 取有限个离散值时，预测问题就成了分类（classification）问题</p>
</li>
<li>
<p>监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）；分类器对新的输入进行预测，称为分类</p>
</li>
<li>
<p>分类问题包括学习和分类两个过程。学习过程中，根据已知的训练数据集利用学习方法学习一个分类器；分类过程中，利用已习得的分类器对新的输入实力进行分类</p>
</li>
<li>
<p>分类问题可以用很多学习方法来解决，比如k近邻、决策树、感知机、逻辑斯谛回归、支撑向量机、朴素贝叶斯法、神经网络等</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_精确率和召回率">精确率和召回率</h5>
<div class="ulist">
<ul>
<li>
<p>评价分类器性能的指标一般是分类准确率（accuracy），它定义为分类器对测试集正确分类的样本数与总样本数之比</p>
</li>
<li>
<p>对于二类分类问题，常用的评价指标是精确率（precision）与召回率（recall）</p>
</li>
<li>
<p>通常以关注的类为正类，其它为负类，按照分类器在测试集上预测的正确与否，会有四种情况出现，它们的总数分别记作：</p>
<div class="ulist">
<ul>
<li>
<p>TP：将正类预测为正类的数目</p>
</li>
<li>
<p>FN：将正类预测为负类的数目</p>
</li>
<li>
<p>FP：将负类预测为正类的数目</p>
</li>
<li>
<p>TN：将负类预测为负类的数目</p>
</li>
</ul>
</div>
</li>
<li>
<p>精确率</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[P=\frac{TP}{TP+FP}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>精确率指的是“所有预测为正类的数据中，预测正确的比例”</p>
<div class="ulist">
<ul>
<li>
<p>召回率</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[R=\frac{TP}{TP+FN}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>召回率指的是“所有实际为正类的数据中，被正确预测找出的比例”</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_回归问题">回归问题</h5>
<div class="imageblock">
<div class="content">
<img src="images/huigui.png" alt="huigui">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>回归问题用于预测输入变量和输出变量之间的关系</p>
</li>
<li>
<p>回归模型就是表示从输入变量到输出变量之间映射的函数</p>
</li>
<li>
<p>回归问题的学习等价于函数拟合：选择一条函数曲线，使其很好地拟合已知数据，并且能够很好地预测未知数据</p>
</li>
<li>
<p>回归问题的分类</p>
<div class="ulist">
<ul>
<li>
<p>按照输入变量的个数：一元回归和多元回归</p>
</li>
<li>
<p>按照模型类型：线性回归和非线性回归</p>
</li>
<li>
<p>回归学习的损失函数 —— 平方损失函数</p>
</li>
<li>
<p>如果选取平方损失函数作为损失函数，回归问题可以用著名的最小二乘法（least squares）来求解</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_模型求解算法学习算法">3.2.8. 模型求解算法（学习算法）</h4>
<div class="ulist">
<ul>
<li>
<p>梯度下降算法</p>
</li>
<li>
<p>牛顿法和拟牛顿法</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_梯度下降算法">梯度下降算法</h5>
<div class="ulist">
<ul>
<li>
<p>梯度下降（gradient descent）是一种常用的一阶优化方法，是求解无约束优化问题最简单、最经典的方法之一</p>
</li>
<li>
<p>梯度方向：函数变化增长最快的方向（变量沿此方向变化时函数增长最快）</p>
</li>
<li>
<p>负梯度方向：函数变化减少最快的方向（变量沿此方向变化时函数减少最快）</p>
</li>
<li>
<p>损失函数是系数的函数，那么如果系数沿着损失函数的负梯度方向变化，此时损失函数减少最快，能够以最快速度下降到极小值</p>
</li>
<li>
<p>沿着负梯度方向迭代，迭代后的\(\theta\)使损失函数\(J(\theta)\)更小：</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\theta = \theta - \alpha  \frac{\partial J(\theta)}{\partial \theta}\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/tiduxiajiang.png" alt="tiduxiajiang">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山谷处。</p>
</li>
<li>
<p>从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解</p>
</li>
<li>
<p>如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_牛顿法和拟牛顿法">3.2.9. 牛顿法和拟牛顿法</h4>
<div class="sect4">
<h5 id="_牛顿法">牛顿法</h5>
<div class="paragraph">
<p>迭代公式：</p>
</div>
<div class="stemblock">
<div class="content">
\[x^{(k+1)}=x^{(k)}-H^{-1}_k g_k\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>其中\(g_k=g(x^{(k)})=\nabla f(x^{(k)})\)是f(x) 的梯度向量在\(x^{(k)}\)的值，</p>
</li>
<li>
<p>\(H(x^{(k)})\)是f(x)的海塞矩阵在\(x^{(k)}\)的值</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[H(x)=[\frac{\partial^2 f}{\partial x_i \partial x_j}]_{n \times n}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>梯度下降法只考虑了一阶导数，而牛顿法考虑了二阶导数，因此收敛速度更快</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_拟牛顿法">拟牛顿法</h5>
<div class="ulist">
<ul>
<li>
<p>牛顿法需要求解目标函数的海赛矩阵的逆矩阵，计算比较复杂</p>
</li>
<li>
<p>拟牛顿法通过正定矩阵近似海赛矩阵的逆矩阵，从而大大简化了计算过程</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_机器学习模型简介">4. 机器学习模型简介</h2>
<div class="sectionbody">
<div class="paragraph">
<p>主要内容</p>
</div>
<div class="paragraph">
<p><strong>监督学习</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>回归模型</p>
<div class="ulist">
<ul>
<li>
<p>线性回归</p>
</li>
</ul>
</div>
</li>
<li>
<p>分类模型</p>
<div class="ulist">
<ul>
<li>
<p>感知机</p>
</li>
<li>
<p>K近邻（KNN）</p>
</li>
<li>
<p>决策树</p>
</li>
<li>
<p>逻辑斯蒂回归</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>无监督学习</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>聚类</p>
<div class="ulist">
<ul>
<li>
<p>K均值聚类</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_线性回归">4.1. 线性回归</h3>
<div class="ulist">
<ul>
<li>
<p>线性回归（linear regression）是一种线性模型，它假设输入变量x和单个输出变量y之间存在线性关系</p>
</li>
<li>
<p>具体来说，利用线性回归模型，可以从一组输入变量x的线性组合中，计算输出变量y</p>
</li>
<li>
<p>一元线性回归(只有一个特征\(x\))</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[y=wx+b\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>多元线性回归(有多个特征\(x_0,x_1,\dots,x_n\))</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b\]
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/xianxinghuigui.png" alt="xianxinghuigui">
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
一元线性回归就是从上图中的一堆散点图，拟合出一条直线\(y=wx+b\)。这里的拟合其实就是机器学习。散点为训练数据，我们要从训练数据中学习出一条直线。
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>这里的模型就是线性回归模型。那么机器学习的算法呢？</p>
</div>
<div class="ulist">
<ul>
<li>
<p>最小二乘法</p>
</li>
<li>
<p>梯度下降法</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
注意我们这里还没有讲监督学习三要素中的策略呢！
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_损失函数_2">4.1.1. 损失函数</h4>
<div class="paragraph">
<p>这里使用平方损失函数，并且没有正则化项。</p>
</div>
<div class="stemblock">
<div class="content">
\[L(w,b) = \sum_{i=1}^m (wx_i + b - y_i)^2\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_学习算法">4.1.2. 学习算法</h4>
<div class="paragraph">
<p>求解\(w,b\)使得平方损失函数取得最小值。</p>
</div>
<div class="ulist">
<ul>
<li>
<p>最小二乘法</p>
</li>
<li>
<p>梯度下降法</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_最小二乘法">4.1.3. 最小二乘法</h4>
<div class="ulist">
<ul>
<li>
<p>基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method）。</p>
</li>
<li>
<p>它的主要思想就是选择未知参数，使得理论值与观测值之差的平方和达到最小。</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/zuixiaoercheng.png" alt="zuixiaoercheng">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>我们假设输入属性（特征）的数目只有一个：就是\(f(x)=wx+b\)中的\(x\)。我们的训练数据是\((x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\)。要从这些训练数据中学习一个模型出来，使得：\(f(x_i)=wx_i+b, f(x_i) \approx y_i\)。</p>
</li>
<li>
<p>在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{align}
(w^*,b^*) &amp;= \arg \min_{(w,b)}\sum_{i=1}^m (f(x_i)-y_i)^2 \\
          &amp;= \arg \min_{(w,b)}\sum_{i=1}^m (wx_i+b-y_i)^2
\end{align}\]
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
以上就是监督学习三要素中的策略！
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="_最小二乘法求解过程">最小二乘法求解过程</h5>
<div class="ulist">
<ul>
<li>
<p>求解\(w\)和\(b\)，使得\(E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2\)最小化的过程，称为线性回归模型的“最小二乘估计”。</p>
</li>
<li>
<p>将\(E_{(w,b)}\)分别对\(w\)和\(b\)进行求导，可以得到：</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\begin{align}
\frac{\partial E_{(w,b)}}{\partial w}
&amp;= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&amp;= 2(wx_1+b-y_1)x_1+ \dots +2(wx_m+b-y_m)x_m \\
&amp;= 2(wx_1^2-(y_1-b)x_1) + \dots + 2(wx_m^2-(y_m-b)x_m) \\
&amp;= 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\

\frac{\partial E_{(w,b)}}{b}
&amp;= \frac{\partial ((wx_1+b-y_1)^2+ \dots + (wx_m+b-y_m)^2)}{\partial w} \\
&amp;= 2(wx_1+b-y_1) + \dots + 2(wx_m+b-y_m) \\
&amp;= 2(mb-\sum_{i=1}^m (y_i-wx_i))
\end{align}\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>令偏导数都为0，可以得到：</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[2(mb-\sum_{i=1}^m (y_i-wx_i)) = 0 \\
b = \frac{1}{m}\sum_{i=1}^m (y_i-wx_i)\]
</div>
</div>
<div class="paragraph">
<p>将\(b\)代入到第一个式子中，可以解得\(w\)的值，然后再将\(w\)的值代入到b的表达式，即可解得\(b\)。</p>
</div>
<div class="stemblock">
<div class="content">
\[w = \frac{\sum_{i=1}^m y_i(x_i-\bar{x})}{\sum_{i=1}^m x_i^2-\frac{1}{m}(\sum_{i=1}^m x_i)^2} \\
b = \frac{1}{m}\sum_{i=1}^m (y_i-wx_i)\]
</div>
</div>
<div class="paragraph">
<p>其中：\(\bar{x}=\frac{1}{m}\sum_{i=1}^m x_i\)</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_梯度下降法求解过程">4.1.4. 梯度下降法求解过程</h4>
<div class="paragraph">
<p>求解\(w\)和\(b\)，使得\(E_{(w,b)}=\sum_{i=1}^m (wx_i+b-y_i)^2\)达到极小值，也可以使用梯度下降法。</p>
</div>
<div class="paragraph">
<p>梯度为：</p>
</div>
<div class="stemblock">
<div class="content">
\[\nabla E =
[\frac{\partial}{\partial w} E_{(w,b)}, \frac{\partial}{\partial b} E_{(w,b)}] \\
\frac{\partial E_{(w,b)}}{\partial w} = 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \\
\frac{\partial E_{(w,b)}}{b} = 2(mb-\sum_{i=1}^m (y_i-wx_i))\]
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
这里不是令偏导数为零了，而是准备梯度下降。
</td>
</tr>
</table>
</div>
<div class="stemblock">
<div class="content">
\[w_{next} := w_{current} - \alpha \frac{\partial E}{\partial w} \\
b_{next} := b_{current} - \alpha \frac{\partial E}{\partial b}\]
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
以上，\(w,b\)的初始值，\(\alpha\)的值，以及迭代的次数，都是超参数。因为不是机器学习学习出来的参数。
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>\(\alpha\)在梯度下降算法中被称作为学习率或者步长</p>
</li>
<li>
<p>这意味着我们可以通过\(\alpha\)来控制每一步走的距离，以保证不要走太快，错过了最低点；同时也要保证收敛速度不要太慢</p>
</li>
<li>
<p>所以\(\alpha\)的选择在梯度下降法中往往是很重要的，不能太大也不能太小</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/xuexilv.png" alt="xuexilv">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_梯度下降法和最小二乘法">4.1.5. 梯度下降法和最小二乘法</h4>
<div class="ulist">
<ul>
<li>
<p>相同点</p>
<div class="ulist">
<ul>
<li>
<p>本质和目标相同：两种方法都是经典的学习算法，在给定已知数据的前提下利用求导算出一个模型（函数），使得损失函数最小，然后对给定的新数据进行估算预测</p>
</li>
</ul>
</div>
</li>
<li>
<p>不同点</p>
<div class="ulist">
<ul>
<li>
<p>损失函数：梯度下降可以选取其它损失函数，而最小二乘一定是平方损失函数</p>
</li>
<li>
<p>实现方法：最小二乘法是直接求导找出全局最小；而梯度下降是一种迭代法</p>
</li>
<li>
<p>效果：最小二乘找到的一定是全局最小，但计算繁琐，且复杂情况下未必有解；梯度下降迭代计算简单，但找到的一般是局部最小，只有在目标函数是凸函数时才是全局最小；到最小点附近时收敛速度会变慢，且对初始点的选择极为敏感</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_感知机perceptron">4.2. 感知机（Perceptron）</h3>
<div class="sect3">
<h4 id="_感知机的定义">4.2.1. 感知机的定义</h4>
<div class="paragraph">
<p>感知机是机器学习应用中分类的最简单的一种算法。如下图所示：感知机划分超平面</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/ganzhiji1.png" alt="ganzhiji1">
</div>
</div>
<div class="paragraph">
<p>感知机是二分类的线性模型，输入是实例的特征向量，输出是实例的类别；假设训练的数据集是线性可分的，感知机的目标就是求一个能够将训练集的正负样本完全正确分离开的超平面(也就是上图中所示的那些将蓝、黄数据点完全分离开的直线)。但是如果这些数据是非线性可分的，这个超平面是无法获取的。上图的坐标轴，横坐标为\(X_1\)，纵坐标为\(X_2\)。图中的每一个点都由\((X_1,X_2)\)所决定。举个实例：有一批零件，判断零件是否合格有两个重要点，长度和重量。\(X_1\)表示长度，\(X_2\)表示重量，上图的两条黑线表示零件的长度均值和重量均值。只有当长度和重量都满足一定条件，该零件才为合格品。都不满足一定条件，视为不可修复的劣质品，直接丢弃。那么机器学习如何学习到这个规则呢？我们在代码实现的时候，拿到手的是所有样本的信息\((X_1,X_2)\)和标签(0或1)，标签里面0表示不合格品，1表示合格品。简单的说就是图片上黄色和蓝色的点。根据我们手上的这些点，我们需要找到一条直线将上面的点完美的分开。这样的直线我们可以找到很多条，那么哪一条才是最好的呢？实际上，感知机只是一个二分类的问题，无法找到一条最佳的直线，只需要能把所有的点都分开就好。我们设定损失函数为所有分错的点和直线的距离求和，然后训练，使这段求和的数值最小(最优的情况是0，因为0代表完全分开了)，那么这条直线就满足我们的条件，就是我们所找的。</p>
</div>
</div>
<div class="sect3">
<h4 id="_感知机的数学原理">4.2.2. 感知机的数学原理</h4>
<div class="paragraph">
<p>首先，点\(P(x_0,y_0)\)到直线\(Ax+By+C=0\)的距离为：</p>
</div>
<div class="stemblock">
<div class="content">
\[d=\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}\]
</div>
</div>
<div class="paragraph">
<p>类似的：设超平面公式为：\(h=wx+b\)，其中\(w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)\)。其中样本点\(x'\)到超平面的距离为：</p>
</div>
<div class="stemblock">
<div class="content">
\[d=\frac{wx'+b}{\parallel w \parallel}\]
</div>
</div>
<div class="paragraph">
<p>那么这个超平面为什么设置为\(wx+b\)呢？它和我们常见的\(ax+b\)有什么区别呢？</p>
</div>
<div class="paragraph">
<p>本质没啥区别，\(ax+b\)是二维中的，\(wx+b\)是高维中的。就看你的理解啦，简单的来说，\(wx+b\)是一个\(n\)维空间中的超平面\(S\)，其中\(w\)是超平面的法向量，\(b\)是超平面的截距，这个超平面将特征空间划分成两部分，位于两部分的点分别被分为正负两类，所以，超平面S称为分离超平面。其中\(w=(w_0,w_1,w_2,\dots,w_n),x=(x_0,x_1,x_2,\dots,x_n)\)。</p>
</div>
<div class="paragraph">
<p>细节：</p>
</div>
<div class="paragraph">
<p>\(w\)是超平面的法向量：对于一个平面来说\(w\)就是这么定义的。数学上就这么定义的。</p>
</div>
<div class="paragraph">
<p>\(b\)是超平面的截距：可以按照二维中的\(ax+b\)理解。</p>
</div>
<div class="paragraph">
<p>特征空间：也就是整个\(n\)维空间，样本的每个属性都叫一个特征，特征空间的意思就是在这个空间中可以找到样本所有的属性组合。</p>
</div>
</div>
<div class="sect3">
<h4 id="_感知机的模型">4.2.3. 感知机的模型</h4>
<div class="imageblock">
<div class="content">
<img src="images/ganzhiji2.png" alt="ganzhiji2">
</div>
</div>
<div class="paragraph">
<p>感知机的模型：输入空间—&gt;输出空间：</p>
</div>
<div class="stemblock">
<div class="content">
\[f(x)=sign(wx+b), 其中, sign(x)=
\begin{cases}
  -1 &amp; x &lt; 0 \\
  1 &amp; x \ge 0
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>sign函数很简单，当x大于等于0，sign输出1，否则输出-1。那么往前想一下，\(wx+b\)如果大于等于0，\(f(x)\)就等于1，反之\(f(x)\)等于-1。</p>
</div>
</div>
<div class="sect3">
<h4 id="_感知机的损失函数">4.2.4. 感知机的损失函数</h4>
<div class="paragraph">
<p>我们定义样本\((x_i,y_i)\)，如果上面的距离\(d &gt; 0\)，则\(y_i=1\)；如果\(d &lt; 0\),则\(y_i=-1\)，这样取\(y\)有一个好处，就是方便定义损失函数。优化的目标：期望使误分类的所有样本，到超平面的距离之和最小。</p>
</div>
<div class="paragraph">
<p>所以定义损失函数为：</p>
</div>
<div class="stemblock">
<div class="content">
\[L(w,b)=-\frac{1}{\parallel w \parallel} \sum_{x_i \in M}y_i(wx_i+b)\]
</div>
</div>
<div class="paragraph">
<p>其中M集合就是误分类点的集合。</p>
</div>
<div class="paragraph">
<p>不考虑前面的系数，感知机模型的损失函数为：</p>
</div>
<div class="stemblock">
<div class="content">
\[L(w,b)=-\sum_{x_i \in M}y_i(wx_i+b)\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_感知机学习算法">4.2.5. 感知机学习算法</h4>
<div class="paragraph">
<p>感知机学习算法是对于上述损失函数进行极小化，求得\(w\)和\(b\)。这里使用随机梯度下降法(SGD)，因为误分类的M集合里面的样本才能参加损失函数的优化。</p>
</div>
<div class="paragraph">
<p>目标函数如下：</p>
</div>
<div class="stemblock">
<div class="content">
\[L(w,b)=\arg \min_{w,b}(-\sum_{x_i \in M}y_i(wx_i+b))\]
</div>
</div>
<div class="paragraph">
<p><strong>算法步骤</strong></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>输入：训练数据集\(T=(x_N,y_N),(x_N,y_N),\dots,(x_N,y_N),y_i \in \{-1,+1\},学习率 \eta (0 &lt; \eta &lt; 1)\)</p>
</div>
<div class="paragraph">
<p>输出：\(w,b\)；感知机模型\(f(x)=sign(wx+b)\)</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>赋初值\(w_0,b_0\)</p>
</li>
<li>
<p>选取数据点\((x_i,y_i)\)</p>
</li>
<li>
<p>判断该数据点是否为当前模型的误分类点，即判断若\(y_i(wx_i+b) \le 0\)则更新：</p>
</li>
</ol>
</div>
<div class="stemblock">
<div class="content">
\[w = w + \eta y_i x_i \\
b = b + \eta y_i\]
</div>
</div>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>转到2，直到训练集中没有误分类点。</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_代码">4.2.6. 代码</h4>
<div class="paragraph">
<p><a href="https://zhuanlan.zhihu.com/p/32925500">点击链接学习如何安装和使用Anaconda</a></p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 1.0<br>
Last updated 2019-09-11 21:49:03 +0800
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>